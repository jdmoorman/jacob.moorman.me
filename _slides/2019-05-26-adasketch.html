---
layout: presentation
title: Comparing Sketch-and-Project Methods (Advancement to Candidacy)
order: 8
---

## Comparing Adaptive Sketch-and-Project Methods

Jacob Moorman




-vertical-

In collaboration with

*Denali Molitor* and *Deanna Needell* of UCLA

and *Robert Gower* of Telecom-Paristech




-vertical-

Jacob was funded by NSF grant DGE-1829071

\\(
\def\range{\mathrm{Range}}
\def\eqdef{\overset{\mathrm{def}}{=}}
\def\sigi{\sigma\_i}
\def\sigmin{\sigma\_\min}
\def\sigmax{\sigma\_\max}
\def\t{\top}
\def\R{\mathbb{R}}
\def\P{\mathbb{P}}
\def\E{\mathbb{E}}
\def\Earg#1{\E\left[#1\right]}
\def\Ephiarg#1{\E\_\phi\left[#1\right]}
\def\r{\mathbf{r}}
\def\rnext{\r^{k+1}}
\def\rcurr{\r^{k}}
\def\rexact{\r^\star}
\def\y{\mathbf{y}}
\def\x{\mathbf{x}}
\def\xinit{\x^{0}}
\def\xnext{\x^{k+1}}
\def\xcurr{\x^{k}}
\def\xexact{\x^{\star}}
\def\e{\mathbf{e}}
\def\einit{\xinit - \xexact}
\def\enext{\xnext - \xexact}
\def\ecurr{\xcurr - \xexact}
\def\ik{ {i\_k} }
\def\D{\mathbf{D}}
\def\A{\mathbf{A}}
\def\barA{\bar{\A}}
\def\Ai{\A\_i}
\def\barAi{\barA\_i}
\def\Ao{\A\_1}
\def\Am{\A\_m}
\def\Aik{\A\_\ik}
\def\barAik{\barA\_\ik}
\def\pmat{\mathbf{P}}
\def\pmatik{\pmat\_\ik}
\def\bmat{\mathbf{Q}}
\def\bmatik{\bmat\_\ik}
\def\N{\mathbf{N}}
\def\Nk{\N\_k}
\def\B{\mathbf{B}}
\def\Binv{\B^{-1}}
\def\S{\mathbf{S}}
\def\Sk{\S\_k}
\def\I{\mathbf{I}}
\def\W{\mathbf{W}}
\def\Wk{\W\_k}
\def\Z{\mathbf{Z}}
\def\Zk{\Z\_k}
\def\b{\mathbf{b}}
\def\bi{\b\_i}
\def\bik{\b\_\ik}
\def\barb{\bar{\mathbf{b}}}
\def\barbi{\barb\_i}
\def\barbik{\barb\_\ik}
\def\Pk{\P\_k}
\def\Pik{ \P(\ik=i \mid \xcurr) }
\def\Pok{ \P(\ik=1 \mid \xcurr) }
\def\Pmk{ \P(\ik=m \mid \xcurr) }
\def\PSk{ \P(\Sk=\S \mid \xcurr) }
\def\argmin#1{\underset{#1}{\arg\min}}
\def\argmax#1{\underset{#1}{\arg\max}}
\def\proj#1{\underset{#1}{\text{proj}}}
\def\diag{\text{diag}}
\def\bproj#1{\underset{#1}{\text{proj}_\B}}
\def\norm#1{\left\lVert#1\right\rVert}
\def\bnorm#1{\left\lVert#1\right\rVert_B}
\def\abs#1{\left|#1\right|}
\def\trace#1{\mathbf{Tr}\left(#1\right)}
\def\innerprod#1#2{\langle #1 , #2 \rangle}
\\)

\\(
\def\sigphi{\kappa\_\phi(\A)}
\def\sigphisq{\kappa\_\phi^{-2}(\A)}
\\)




-horizontal-

## Problem

Given \\(\A \in \R^{m \times n}\\) and \\(\b \in \R^m\\),

solve for \\(\xexact\\) satisfying \\(\A\xexact=\b\\)

such that \\(\xexact\\) has minimal norm.




-vertical-

## Notation

It will frequently be useful to use

\\[\D \eqdef \diag(\norm{\A}\_1, \ldots, \norm{\A}\_m)\\]
\\[\bar{\b} \eqdef \D^{-1} \b\\]
\\[\bar{\A} \eqdef \D^{-1} \A\\]




-horizontal-

## Kaczmarz Methods

Make an initial guess \\(\xinit \in \range(\A)\\)

1. Select a row index \\(\ik \in [m]\\) by some **selection rule**

2. Update \\(\xnext = \argmin{\x \in \R^n} \norm{\x - \xcurr}\\) s.t. \\(\Aik \x = \bik\\)

3. Repeat until convergence




-vertical-

## Static Selection Rules

Row index \\(\ik\\) sampled i.i.d. at each iteration.

* \\(\Pik = \norm{\Ai}^2/\norm{\A}_F^2\\)

* \\(\Pik = 1/m\\)




-vertical-

## Adaptive Selection Rules

Choice of row index \\(\ik\\) depends on \\(\xcurr\\)

* **Max distance (MD)**: \\(\ik = \argmax{i}\norm{\xnext - \xcurr}\\)

* **Sampling MD**: \\(\ik = \argmax{i\in \tau\_k}\norm{\xnext - \xcurr}\\)

* \\(\Pik = \abs{\bi - \Ai \xcurr}^2/\norm{\b -  \A \xcurr}^2\\)

* greedy randomized rules [BW18]




-vertical-

## Convergence

Let \\(\phi\\) parameterize the selection rule.

There exists a constant \\(\sigphi\\) such that

\\[\Ephiarg{\norm{\ecurr}^2} \le (1 - \sigphisq)^k \norm{\einit}^2.\\]




-vertical-

## Convergence

The smallest such constant \\(\sigphi\\) can be expressed as

\\[\def\grossp{\bar{\pmat}\_\phi(\e, \A)}
\sigphisq = \min\_{\e \in \range(\A^\t)} \left(\frac{\e^\t\grossp\e}{\norm{\e}^2}\right),\\]

where

\\[\grossp = \E\_{\phi}\left[\barAik^\t \barAik \mid \xcurr=\xexact+\e \right].\\]




-horizontal-

## Convergence Results

Many papers present a selection rule and

compute or bound \\(\sigphi\\) for that rule.




-vertical-

## The original Randomized Kaczmarz [SV09]

\\[\Pik = \norm{\Ai}^2/\norm{\A}_F^2\\]

\\[\sigphisq = \frac{\sigmin^2(\A)}{\sum\_i \sigi^2(\A)}\\]




-vertical-

## Max Distance [NSLSKV16]

\\[\ik = \argmax{i}\norm{\xnext - \xcurr} = \argmin{i}\norm{\xnext - \xexact}\\]

\\[\sigphisq = \min\_{\e \in \range(\A^\t)} \left(\frac{\norm{\barA\e}\_\infty^2}{\norm{\e}^2}\right)\\]




-vertical-

## Sampling Max Distance [DHN17]

Sample some row indices \\(\tau\_k \subset [m]\\),

then let \\(\ik = \argmax{i\in \tau\_k}\norm{\xnext - \xcurr}\\).

\\[\sigphisq \geq \frac{m}{V\_k}\frac{\sigmin^2(\A)}{\sum\_i \sigi^2(\A)}\\]





-vertical-

## Greedy Randomized Kaczmarz [BW18]

\\(\Pik \propto \abs{\barbi - \barAi \xcurr}^2\\) for

\\(\abs{\barbi - \barAi \xcurr}^2 \geq \theta \norm{\barb - \barA \xcurr}\_\infty^2 + (1-\theta)\frac{1}{m}\norm{\barb - \barA \xcurr}^2\\),

and \\(\Pik = 0\\) otherwise.

\\[\sigphisq \geq \frac{1}{2}\left(\frac{1}{\gamma}\norm{\A}\_F^2 + 1\right)\frac{\sigmin^2(\A)}{\sum\_i \sigi^2(\A)} \overset{O(1/m)}{\approx} \frac{\sigmin^2(\A)}{\sum\_i \sigi^2(\A)}\\]




-horizontal-

## Contributions

* MLRP implies ordering
* Thus, if two methods are the same cost, the more aggressive one wins.




-vertical-

## Generalization

Same result holds on sketch-and-project class of methods, which includes coordinate descent, block coordinate descent, block kaczmarz as special cases.




-horizontal-

## Future Work

* Exact MSE analysis inspired by []
* Publish papers
* Publish Averaged Kaczmarz stuff
* Generalize Averaged Kaczmarz to Sketch-and-Project
* Extend everything to the inconsistent case




-horizontal-

## References

<ul style="font-size:24px !important;">
  <li>
[SV09] T. Strohmer and R. Vershynin, *"A randomized Kaczmarz algorithm with exponential convergence"*, J. Fourier Anal. Appl., 15 (2009), pp. 262–278.
  </li>
  <li>
[NSLSKV16] J. Nutini, B. Sepehry, I. Laradji, M. Schmidt, H. Koepke, and A. Virani. *"Convergence rates for greedy Kaczmarz algorithms, and faster randomized Kaczmarz rules using the orthogonality graph."* In Proceedings of the Thirty-Second Conference on Uncertainty in Artificial Intelligence, pages 547–556. AUAI Press, 2016
  </li>
  <li>
[DHN17]
  </li>
  <li>
[BW18] Z.-Z. Bai and W.-T. Wu, *"On greedy randomized Kaczmarz method for solving large sparse linear systems"*, SIAM J. Sci. Comput., vol. 40, no. 1, pp. A592–A606, 2018.
  </li>
  <li>

  </li>
  <li>

  </li>
  <li>

  </li>
</ul>





-horizontal-

# Junk




-vertical-

## Kaczmarz Update Formula

\\[
\begin{align}\xnext &= \argmin{\x \in \R^n} \norm{\x - \xcurr} \text{ s.t. } \Aik \x = \bik \\\\
&= \proj{\\\{\x\ :\ \Aik \x = \bik\\\}}(\xcurr)\\\\
&= \xcurr - \frac{\Aik \xcurr - \bik}{\norm{\Aik}^2}\Aik^\t
\end{align}\\]




-vertical-

## Kaczmarz Convergence

The update from \\(\xcurr\\) to \\(\xnext\\) can be written explicitly

\\[
\xnext = \left(\I - \frac{\Aik^\t\Aik}{\norm{\Aik}^2}\right) \xcurr + \frac{\bik \Aik^\t}{\norm{\Aik}^2}
\\]




-vertical-

## Kaczmarz Convergence

From which the error update can be written explicitly

\\[
\begin{align}
\enext &= \left(\I - \frac{\Aik^\t\Aik}{\norm{\Aik}^2}\right) \ecurr + \frac{\rexact\_\ik \Aik^\t}{\norm{\Aik}^2} \\\\
&= \left(\I - \frac{\Aik^\t\Aik}{\norm{\Aik}^2}\right) \ecurr
\end{align}
\\]

Where \\(\ecurr \eqdef \xcurr - \xexact\\) and \\(\rexact \eqdef \b - \A\xexact\\)




-vertical-

## Kaczmarz Convergence

\\[
\enext = \left(\I - \frac{\Aik^\t\Aik}{\norm{\Aik}^2}\right) \ecurr + \frac{\rexact\_\ik \Aik^\t}{\norm{\Aik}^2}
\\]

The terms are orthogonal wrt \\(\innerprod{\cdot}{\cdot}\\), so

\\[
\begin{align}
\norm{\enext}^2 &= \norm{\left(\I - \frac{\Aik^\t\Aik}{\norm{\Aik}^2}\right) \ecurr}^2 + \norm{\frac{\rexact\_\ik \Aik^\t}{\norm{\Aik}^2}}^2 \\\\
&= \norm{\ecurr}^2 - \norm{\frac{\Aik^\t\Aik}{\norm{\Aik}^2} \ecurr}^2 + \frac{\abs{\rexact\_\ik}^2}{\norm{\Aik}^2}.
\end{align}
\\]




-vertical-

## Kaczmarz Convergence

Finally, we have the error norm update

\\[
\norm{\enext}^2 = \norm{\ecurr}^2 - \frac{\abs{\Aik \ecurr}^2}{\norm{\Aik}^2} + \frac{\abs{\rexact\_\ik}^2}{\norm{\Aik}^2}.
\\]




-vertical-

## Kaczmarz Convergence

\\[\D := \diag\left(\norm{\Ao}^2,\ \dots,\ \norm{\Am}^2\right)\\]
\\[\Pk := \diag\left(\Pok,\ \dots,\ \Pmk\right)\\]

\\[
\begin{align}
&\E\left[\norm{\enext}^2 \mid \ecurr\right] \\\\
&= \norm{\ecurr}^2 - \norm{\Pk \D^{-1} \A \ecurr}^2 + \norm{\Pk \D^{-1} \rexact}^2.
\end{align}
\\]




-vertical-

## Kaczmarz Bounds

Look for \\(\alpha, \beta\\) independent of \\(\ecurr\\) for which

\\[
\begin{align}
&\E\left[\norm{\enext}^2 \mid \ecurr\right] \\\\
&= \norm{\ecurr}^2 - \norm{\Pk \D^{-1} \A \ecurr}^2 + \norm{\Pk \D^{-1} \rexact}^2 \\\\
&\le \left(1 - \alpha^2\right)\norm{\ecurr}^2 + \beta^2
\end{align}
\\]




-vertical-

## Kaczmarz Bounds

\\[
\begin{align}
&\E\left[\norm{\ecurr}^2\right] \\\\
&\le \left(1 - \alpha^2\right)\E\left[\norm{\eprev}^2\right] + \beta^2 \\\\
&\le \dots \\\\
&\le \left(1 - \alpha^2\right)^k\norm{\einit}^2 + \sum_{l = 0}^{k-1}\left(1 - \alpha^2\right)^l \beta^2 \\\\
&\le \left(1 - \alpha^2\right)^k\norm{\einit}^2 + \frac{\beta^2}{\alpha^2}.
\end{align}
\\]




-vertical-

Require for any \\(\ecurr\\) where \\(\A\ecurr \neq 0\\) that

\\[
\alpha \norm{\ecurr} \le \norm{\Pk \D^{-1}\A\ecurr}
\\]

\\[
\norm{\Pk \D^{-1}\rexact} \le \beta.
\\]

Leading to the natural choices

\\[
\alpha = \underset{\A\ecurr \neq 0}{\inf}\frac{\norm{\Pk \D^{-1}\A\ecurr}}{\norm{\ecurr}}
\\]

\\[
\beta = \underset{\A\ecurr \neq 0}{\sup}\norm{\Pk \D^{-1}\rexact}
\\]




-horizontal-

## Generalization: Sketch and Project (S&P) Methods

Fix \\(\B \in \R^{n \times n}\\), symmetric positive definite.

1. Select \\(\Sk \in \R^{m \times \tau_k}\\) according to some **selection rule**

2. Let \\(\xnext = \argmin{\x\in\R^n} \norm{\x - \xcurr}_B\\) s.t. \\(\Sk^\t \A \x = \Sk^\t\b\\)

3. Repeat until convergence




-vertical-

## S&P Selection Rules

Options for \\(\PSk\\)

* **Static Distribution**:
Sample \\(\Sk\\) i.i.d. from a fixed distribution

    * \\(\PSk \propto \trace{\S^\t\A\Binv \A^\t \S}\\)

    * \\(\text{uniform}\left(\\\{\S^{(1)}, \dots, \S^{(r)} \\\}\right)\\)




-vertical-

## S&P Selection Rules

* **Adaptive Distribution**:
Sample \\(\Sk\\) from a distribution which depends on \\(\xcurr\\)

    * Includes static distributions as a special case

    * **Max residual (MR)**: \\(\Sk = \argmax{\S}\norm{\S^\t (\A \xcurr - \b)}\\)

    * **Max distance (MD)**: \\(\Sk = \argmax{\S}\norm{\xnext - \xcurr}_B\\)




-vertical-

## S&P Update Formula

\\[
\begin{align}
\xnext &= \argmin{\x \in \R^n} \norm{\x - \xcurr}_B \text{ s.t. } \Sk^\t \A \x = \Sk^\t\b\\\\
&= \bproj{\\\{\x\ :\ \Sk^\t\A \x = \Sk^\t\b\\\}}(\xcurr)\\\\
&= \xcurr - \Binv \A^\t \Sk \left(\Sk^\t \A \Binv \A^\t \Sk\right)^\dagger \Sk^\t (\A \xcurr - \b)
\end{align}
\\]




-vertical-

## S&P Update Formula

Define \\(\Wk = \Sk \left(\Sk^\t \A \Binv \A^\t \Sk\right)^\dagger \Sk^\t\\),

and \\(\Zk = \A^\t \Wk \A\\).

The S&P update can then be written

\\[\xnext = \xcurr - \Binv \Zk \xcurr + \Binv \A^\t \Wk \b.\\]




-vertical-

## S&P Convergence

\\[
\xnext = \xcurr - \Binv \Zk \xcurr + \Binv \A^\t \Wk \b
\\]

Define \\(\ecurr = \xcurr - \xexact\\), and \\(\rexact = \b - \A\xexact\\)

\\[
\begin{align}
\enext &= \ecurr - \Binv \Zk \ecurr + \Binv \A^\t \Wk \rexact \\\\
&= \left( \I - \Binv \Zk \right) \ecurr + \Binv \A^\t \Wk \rexact
\end{align}
\\]




-vertical-

## S&P Convergence

\\[
\enext = \left( \I - \Binv \Zk \right) \ecurr + \Binv \A^\t \Wk \rexact
\\]

The terms are orthogonal wrt \\(\innerprod{\cdot}{\cdot}_B\\), so

\\[
\begin{align}
\bnorm{\enext}^2 &= \bnorm{\left( \I - \Binv \Zk \right) \ecurr}^2 + \bnorm{\Binv \A^\t \Wk \rexact}^2 \\\\
&= \bnorm{\ecurr}^2 - \bnorm{\Binv \Zk \ecurr}^2 + \bnorm{\Binv \A^\t \Wk \rexact}^2.
\end{align}
\\]




-vertical-

## S&P Convergence

Finally, we have the error norm update

\\[
\begin{align}
\bnorm{\enext}^2 &= \bnorm{\ecurr}^2 - \bnorm{\Binv \Zk \ecurr}^2 + \bnorm{\Binv \A^\t \Wk \rexact}^2 \\\\
&= \bnorm{\ecurr}^2 - \innerprod{\Zk \ecurr}{\ecurr} + \innerprod{\Wk \rexact}{\rexact}.
\end{align}
\\]




-vertical-

## S&P Convergence

\\[
\begin{align}
&\E\left[\norm{\enext}^2 \mid \ecurr\right] \\\\
&= \E\left[\bnorm{\ecurr}^2 - \innerprod{\Zk \ecurr}{\ecurr} + \innerprod{\Wk \rexact}{\rexact}\mid \ecurr\right] \\\\
&= \bnorm{\ecurr}^2 - \innerprod{\E\left[\Zk \mid \ecurr\right] \ecurr}{\ecurr} + \innerprod{\E\left[\Wk \mid \ecurr\right] \rexact}{\rexact}.
\end{align}
\\]





-vertical-

## S&P Bounds

Look for \\(\alpha, \beta\\) independent of \\(\ecurr\\) for which

\\[
\begin{align}
&\E\left[\norm{\enext}^2 \mid \ecurr\right] \\\\
&= \bnorm{\ecurr}^2 - \innerprod{\E\left[\Zk \mid \ecurr\right] \ecurr}{\ecurr} + \innerprod{\E\left[\Wk \mid \ecurr\right] \rexact}{\rexact} \\\\
&\le \left(1 - \alpha^2\right)\bnorm{\ecurr}^2 + \beta^2
\end{align}
\\]




-vertical-

## S&P Bounds

\\[
\begin{align}
&\E\left[\norm{\ecurr}^2\right] \\\\
&\le \left(1 - \alpha^2\right)\E\left[\norm{\eprev}^2\right] + \beta^2 \\\\
&\le \dots \\\\
&\le \left(1 - \alpha^2\right)^k\norm{\einit}^2 + \sum_{l = 0}^{k-1}\left(1 - \alpha^2\right)^l \beta^2 \\\\
&\le \left(1 - \alpha^2\right)^k\norm{\einit}^2 + \frac{\beta^2}{\alpha^2}.
\end{align}
\\]




-vertical-

Require for any \\(\ecurr\\) where \\(\A\ecurr \neq 0\\) that

\\[
\alpha \bnorm{\ecurr} \le \innerprod{\E\left[\Zk \mid \ecurr\right] \ecurr}{\ecurr}
\\]

\\[
\innerprod{\E\left[\Wk \mid \ecurr\right] \rexact}{\rexact} \le \beta.
\\]

Leading to the natural choices

\\[
\alpha = \underset{\A\ecurr \neq 0}{\inf}\frac{\innerprod{\E\left[\Zk \mid \ecurr\right] \ecurr}{\ecurr}}{\bnorm{\ecurr}}
\\]

\\[
\beta = \underset{\A\ecurr \neq 0}{\sup}\innerprod{\E\left[\Wk \mid \ecurr\right] \rexact}{\rexact}
\\]




-horizontal-

## References

* D. Needell, _"Randomized Kaczmarz solver for noisy linear systems,"_ BIT, 50 (2010), pp. 395–403.




-vertical-

## References

* Gower, R.M., Richtarik, P., _"Randomized iterative methods for linear systems."_ SIAM J. Matrix Anal. Appl. 36(4), 1660–1690 (2015)
    * Introduces sketch & project methods, proves convergence for consistent systems




-vertical-

## References

* Yushi Morijiri, Kensuke Aishima, Takayasu Matsuo, _"Extension of an error analysis of the randomized Kaczmarz method for inconsistent linear systems,"_ JSIAM Letters, 2018, 10 巻, p. 17-20
    * Extends convergence of sketch and project methods to inconsistent systems.




-vertical-

## References

* Nutini, J., et al., _"Convergence rates for greedy Kaczmarz algorithms, and faster randomized Kaczmarz rules using the orthogonality graph,"_ Proceedings of the Thirty-Second Conference on Uncertainty in Artificial Intelligence. AUAI Press, Corvallis (2016)
    * Analyses greedy kaczmarz rules applied to consistent systems
