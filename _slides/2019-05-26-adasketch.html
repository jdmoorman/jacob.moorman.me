---
layout: presentation
title: On Comparing Adaptive Kaczmarz Methods (ATC)
order: 8
---

## On Comparing Adaptive Kaczmarz Methods


Jacob Moorman




-vertical-

In collaboration with

*Denali Molitor* and *Deanna Needell* of UCLA

and *Robert Gower* of Telecom-Paristech




-vertical-

Jacob was funded by NSF grant DGE-1829071

\\(
\def\range{\mathrm{Range}}
\def\eqdef{\overset{\mathrm{def}}{=}}
\def\sigi{\sigma\_i}
\def\sigmin{\sigma\_\min}
\def\sigmax{\sigma\_\max}
\def\t{\top}
\def\R{\mathbb{R}}
\def\P{\mathbb{P}}
\def\E{\mathbb{E}}
\def\Earg#1{\E\left[#1\right]}
\def\Ephiarg#1{\E\_\phi\left[#1\right]}
\def\Ephihatarg#1{\E\_{\hat\phi}\left[#1\right]}
\def\r{\mathbf{r}}
\def\rnext{\r^{k+1}}
\def\rcurr{\r^{k}}
\def\rexact{\r^\star}
\def\y{\mathbf{y}}
\def\x{\mathbf{x}}
\def\xinit{\x^{0}}
\def\xnext{\x^{k+1}}
\def\xcurr{\x^{k}}
\def\xexact{\x^{\star}}
\def\e{\mathbf{e}}
\def\einit{\xinit - \xexact}
\def\enext{\xnext - \xexact}
\def\ecurr{\xcurr - \xexact}
\def\ik{ {i\_k} }
\def\D{\mathbf{D}}
\def\A{\mathbf{A}}
\def\Ai{\A\_i}
\def\Ao{\A\_1}
\def\Am{\A\_m}
\def\Aik{\A\_\ik}
\def\pmat{\mathbf{P}}
\def\pmatik{\pmat\_\ik}
\def\bmat{\mathbf{Q}}
\def\bmatik{\bmat\_\ik}
\def\N{\mathbf{N}}
\def\Nk{\N\_k}
\def\B{\mathbf{B}}
\def\Binv{\B^{-1}}
\def\S{\mathbf{S}}
\def\Si{\S\_i}
\def\Hi{\mathbf{H}\_i}
\def\Sk{\S\_k}
\def\I{\mathbf{I}}
\def\W{\mathbf{W}}
\def\Wk{\W\_k}
\def\Z{\mathbf{Z}}
\def\Zk{\Z\_k}
\def\b{\mathbf{b}}
\def\bi{\b\_i}
\def\bik{\b\_\ik}
\def\Pk{\P\_k}
\def\Pik{ \P(\ik=i \mid \xcurr) }
\def\Pok{ \P(\ik=1 \mid \xcurr) }
\def\Pmk{ \P(\ik=m \mid \xcurr) }
\def\PSk{ \P(\Sk=\S \mid \xcurr) }
\def\argmin#1{\underset{#1}{\arg\min}}
\def\argmax#1{\underset{#1}{\arg\max}}
\def\proj#1{\underset{#1}{\text{proj}}}
\def\diag{\text{diag}}
\def\bproj#1{\underset{#1}{\text{proj}_\B}}
\def\norm#1{\left\lVert#1\right\rVert}
\def\bnorm#1{\left\lVert#1\right\rVert_B}
\def\abs#1{\left|#1\right|}
\def\trace#1{\mathbf{Tr}\left(#1\right)}
\def\innerprod#1#2{\langle #1 , #2 \rangle}
\\)

\\(
\def\r{\mathbf{r}}
\def\ri{\r\_i}
\def\rj{\r\_j}
\def\barr{\bar{\r}^k}
\def\barri{\barr\_i}
\def\barrj{\barr\_j}
\def\barrik{\barr\_\ik}
\def\barb{\bar{\mathbf{b}}}
\def\barbi{\barb\_i}
\def\barbik{\barb\_\ik}
\def\barA{\bar{\A}}
\def\barAi{\barA\_i}
\def\barAik{\barA\_\ik}
\def\lelr{\leq\_{\text{wr}}}
\def\sigphihat{\kappa\_{\hat \phi}(\A)}
\def\sigphi{\kappa\_\phi(\A)}
\def\sigphisq{\kappa\_\phi^{-2}(\A)}
\def\sigphihatsq{\kappa\_{\hat \phi}^{-2}(\A)}
\\)




-horizontal-

## Problem

Given \\(\A \in \R^{m \times n}\\) and \\(\b \in \R^m\\),

solve for \\(\xexact\\) satisfying \\(\A\xexact=\b\\)

such that \\(\xexact\\) has minimal norm.




-vertical-

## Notation

We will use an overbar to denote normalization by the row norms of \\(\A\\) using \\(\D \eqdef \diag(\norm{\A\_1}, \ldots, \norm{\A\_m})\\)

\\[\barb \eqdef \D^{-1} \b\\]
\\[\barA \eqdef \D^{-1} \A\\]
\\[\barr \eqdef \barb - \barA \xcurr\\]




-horizontal-

## Kaczmarz Methods

Make an initial guess \\(\xinit \in \range(\A^\t)\\)

1. Select a row index \\(\ik \in [m]\\) by some **selection rule**

2. Update \\(\xnext = \argmin{\x \in \R^n} \norm{\x - \xcurr}\\) s.t. \\(\Aik \x = \bik\\)

3. Repeat until convergence




-vertical-

## Static Selection Rules

Row index \\(\ik\\) sampled i.i.d. at each iteration.

* \\(\Pik = p\_i\\)

* \\(\Pik = \norm{\Ai}^2/\norm{\A}_F^2\\)

* \\(\Pik = 1/m\\)




-vertical-

## Motivation for Adaptive Rules

Then the expected error update is

\\[\begin{align}
\Earg{\norm{\enext}^2 \mid \xcurr} &= \norm{\ecurr}^2 - \Earg{\norm{\xnext - \xcurr}^2\mid \xcurr}\\\\
&=\norm{\ecurr}^2 - \Earg{\abs{\barrik}^2\mid \xcurr}
\end{align}\\]

where \\(\barr = \barb - \barA \xcurr\\).




-vertical-

## Adaptive Selection Rules

Choice of row index \\(\ik\\) depends on \\(\xcurr\\)

* **Max distance (MD)**: \\(\ik = \argmax{i}\norm{\xnext - \xcurr}\\)

* **Sampling MD**: \\(\ik = \argmax{i\in \tau\_k}\norm{\xnext - \xcurr}\\)

* \\(\Pik = \abs{\barri}^2/\norm{\barr}^2\\)




-horizontal-

## Selection Rule Parameterization

Recalling the expected error update

\\[
\Ephiarg{\norm{\enext}^2 \mid \xcurr} =\norm{\ecurr}^2 - \Ephiarg{\abs{\barrik}^2\mid \xcurr}
\\]




-vertical-

## Selection Rule Parameterization

A general way to parameterize the selection rule is

\\[\Pik = \frac{\phi\left(\abs{\barri}^2; i, \barr\right)}{\sum\_j\phi\left(\abs{\barrj}^2; j, \barr\right)} \quad \text{or} \quad \frac{\phi\left(\abs{\barri}^2\right)}{\sum\_j\phi\left(\abs{\barrj}^2\right)}\\]

for some weighting function \\(\phi\\).




-vertical-

## Example Parameterizations

| <span style="font-weight:normal">\\(\phi(\abs{\r\_i}^2; i, \r)\\)</span> ||| Selection Rule |
|:---:|||:---:|
| \\(1\\) ||| Uniform sampling |
| \\(\norm{\A\_i}^2\\) ||| \\(\Pik = \norm{\Ai}^2/\norm{\A}_F^2\\) |
| \\(\mathbb{1}(\abs{\r\_i}=\norm{\r}\_\infty)\\) ||| Max distance rule |
| \\(\norm{\A\_i}^2\\)\\(\mathbb{1}(\abs{\r\_i}\geq\norm{\r}\_\infty)\\) ||| A rule I just made up |




-horizontal-

## Convergence

Given a selection rule parameterization \\(\phi\\),

there exists a rate constant \\(\sigphi\\) such that

\\[\Ephiarg{\norm{\ecurr}^2} \le (1 - \sigphisq)^k \norm{\einit}^2.\\]




-vertical-

## Convergence

Any such rate constant \\(\sigphi\\) should satisfy

\\[\begin{align}\Ephiarg{\abs{\barrik}^2\mid \xcurr} \le \sigphisq \norm{\ecurr}^2\end{align}\\]

for any \\(\xcurr\in \range(\A^\t)\\).




-vertical-

## Convergence

The smallest such rate constant \\(\sigphi\\) can be written as \\(\def\grossp{\bar{\pmat}\_\phi(\e, \A)}\\)

\\[\begin{align}
\sigphisq &= \min\_{\xcurr \in \range(\A^\t)}\left(\frac{\Ephiarg{\abs{\barrik}^2\mid \xcurr}}{\norm{\ecurr}^2}\right)
\end{align}\\]

Which, though not obvious, does not depend on \\(\xexact\\) or \\(k\\).




-horizontal-

## Convergence Results

Many papers present a selection rule and

compute or bound \\(\sigphi\\) for that rule.




-vertical-

## The original Randomized Kaczmarz [SV09]

\\[\Pik = \norm{\Ai}^2/\norm{\A}_F^2\\]

\\[\sigphisq = \frac{\sigmin^2(\A)}{\sum\_i \sigi^2(\A)}\\]




-vertical-

## Max Distance [NSLSKV16]

\\[\ik = \argmax{i}\norm{\xnext - \xcurr} = \argmin{i}\norm{\xnext - \xexact}\\]

\\[\sigphisq = \min\_{\e \in \range(\A^\t)} \left(\frac{\norm{\barA\e}\_\infty^2}{\norm{\e}^2}\right)\\]




-vertical-

## Sampling Max Distance [DHN17]

Sample some row indices \\(\tau\_k \subset [m]\\),

then let \\(\ik = \argmax{i\in \tau\_k}\norm{\xnext - \xcurr}\\).

\\[\sigphisq \geq \frac{m}{V\_k}\frac{\sigmin^2(\A)}{\sum\_i \sigi^2(\A)}\\]

where \\(V\_k = \max (m - s\_k, m - \abs{\tau\_k} + 1)\\)

and \\(s\_k\\) is the number of satisfied equations \\(\Ai \xcurr = \bi\\)




-vertical-

## Greedy Randomized Kaczmarz [BW18]

\\(\Pik \propto \abs{\barri}^2\\) for

\\(\abs{\barri}^2 \geq \theta \norm{\barr}\_\infty^2 + (1-\theta)\frac{1}{m}\norm{\barr}^2\\),

and \\(\Pik = 0\\) otherwise.




-vertical-

## Greedy Randomized Kaczmarz [BW18]

\\[\begin{align}
\sigphisq &\geq \frac{1}{2}\left(\frac{\norm{\A}\_F^2}{\norm{\A}\_F^2 - \min\_i \norm{\Ai}^2} + 1\right)\frac{\sigmin^2(\A)}{\sum\_i \sigi^2(\A)} \\\\
&=  \frac{1}{2}\left(2 - O\(m^{-1})\right)\frac{\sigmin^2(\A)}{\sum\_i \sigi^2(\A)} \\\\
&\approx\frac{\sigmin^2(\A)}{\sum\_i \sigi^2(\A)}
\end{align}\\]




-horizontal-

## Main Result

Suppose \\(\phi\\) and \\(\hat \phi\\) satisfy

\\[\hat \phi \lelr \phi \quad \eqdef \quad \frac{\hat \phi(b)}{\hat \phi(a)} \le \frac{\phi(b)}{\phi(a)} \quad \text{for any}\quad a \le b.\\]

Then \\(\sigphihatsq \le \sigphisq\\).




-vertical-

## Implications

* Given two sampling rules, we can determine which has a faster rate constant \\(\sigphi\\) without evaluating it.

* Max Distance rule has the fastest rate constant \\(\sigphi\\) possible, since \\(\phi \lelr \phi\_\infty\\) for any \\(\phi\\).

* Any monotonically increasing \\(\phi\\) has a faster rate constant \\(\sigphi\\) than static sampling




-vertical-

## Limitations

* Can't compare \\(\Ephihatarg{\norm{\ecurr}^2}\\) to \\(\Ephiarg{\norm{\ecurr}^2}\\)

* Not all \\(\hat \phi\\) and \\(\phi\\) are comparable by \\(\lelr\\)

* Does not give any measure of \\(\sigphisq - \sigphihatsq\\)




-horizontal-

## Future Work

* Wrap up the writing
* What can we say when \\(\hat \phi\\) and \\(\phi\\) are not comparable?
* Compare \\(\Ephiarg{\norm{\ecurr}^2}\\) and \\(\Ephihatarg{\norm{\ecurr}^2}\\)
* Extend to the inconsistent case




-vertical-

## References

<ul style="font-size:24px !important;">
  <li>
**[SV09]** T. Strohmer and R. Vershynin: *"A randomized Kaczmarz algorithm with exponential convergence"*, Journal of Fourier Analysis and Applications, 15 (2009), pp. 262–278.
  </li>
  <li>
**[GR15]** R. Gower and P. Richtarik: *"Randomized Iterative Methods for Linear Systems"*, SIAM Journal on Matrix Analysis and Applications (2015).
  </li>
  <li>
**[NSLSKV16]** J. Nutini, B. Sepehry, I. Laradji, M. Schmidt, H. Koepke, and A. Virani: *"Convergence rates for greedy Kaczmarz algorithms, and faster randomized Kaczmarz rules using the orthogonality graph"*, Proceedings of the Thirty-Second Conference on Uncertainty in Artificial Intelligence, pages 547–556. AUAI Press, 2016.
  </li>
  <li>
**[DHN17]** J.A. De Loera, J. Haddock, D. Needell: *"A sampling Kaczmarz–Motzkin algorithm for linear feasibility"*, SIAM Journal on Scientific Computing, 39(5), S66–S87 (2017).
  </li>
  <li>
**[BW18]** Z.-Z. Bai and W.-T. Wu: *"On greedy randomized Kaczmarz method for solving large sparse linear systems"*, SIAM Journal on Scientific Computing, vol. 40, no. 1, pp. A592–A606, 2018.
  </li>
</ul>




-horizontal-

# More Slides




-vertical-

## Proof Sketch

\\[\begin{align}
&\hat \phi \lelr \phi \quad \\\\
&\implies \P\_{\hat \phi} \leq\_{\text{lr}} \P\_{\phi} & \text{(MLRP)} \\\\
&\implies \P\_{\hat \phi}\left(\abs{\ri}^2 \geq t\right) \leq \P\_{\phi}\left(\abs{\ri}^2 \geq t\right) \quad \forall t & \text{(FOSD)}\\\\
&\implies \E\_{\hat \phi}\left[\abs{\ri}^2\right] \leq \E\_{\phi}\left[\abs{\ri}^2\right]
\end{align}\\]




-vertical-

## Full Main Result

Suppose \\(\phi\\) and \\(\hat \phi\\) satisfy

\\[\hat \phi(\abs{\rj}^2; j, \r)\phi(\abs{\ri}^2; i, \r) \le \phi(\abs{\rj}^2; j, \r)\hat \phi(\abs{\ri}^2; i, \r)\\]

for any \\(i, j\\) with \\(\ri \le \rj\\). Then \\(\sigphihatsq \le \sigphisq\\).




-vertical-

## Sketch-project methods

Fix \\(\B \in \R^{n \times n}\\), symmetric positive definite.

1. Select \\(\Sk \in \R^{m \times \tau_k}\\) according to some **selection rule**

2. Let \\(\xnext = \argmin{\x\in\R^n} \norm{\x - \xcurr}_B\\) s.t. \\(\Sk^\t \A \x = \Sk^\t\b\\)

3. Repeat until convergence




-vertical-

## Sketch-project Residuals

The main result holds on all sketch-project methods where the sketching matrices are sampled from a finite set. The role of \\(\abs{\barri}^2\\) must be changed to \\(\norm{\b - \A \xcurr}^2\_{\Hi}\\), where \\(\Hi = \Si (\Si^\t \A \B^\dagger \A^\t \Si)^{\dagger} \Si^\t\\). Then, the main result holds directly.

This includes coordinate descent, block kaczmarz, and other methods as special cases.




-vertical-

## Experimental Results

<img style="margin:0;"  src="example.png" width="70%">
